<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-01-19T11:14:41-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Sumanyu Muku</title><subtitle>Sumanyu's Portfolio Page.
</subtitle><entry><title type="html">Feature Engineering + IMPALA for Google Research Football</title><link href="http://localhost:4000/blog/2021/DRL/" rel="alternate" type="text/html" title="Feature Engineering + IMPALA for Google Research Football" /><published>2021-12-13T10:09:00-05:00</published><updated>2021-12-13T10:09:00-05:00</updated><id>http://localhost:4000/blog/2021/DRL</id><content type="html" xml:base="http://localhost:4000/blog/2021/DRL/"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Google Research released their <a href="https://ai.googleblog.com/2019/06/introducing-google-research-football.html">Football Environment</a> in 2019 for research in Deep Reinforcement Learning (RL). The environment provides a realistic 3D simulation of the game of football in which the agents can control either a single player or all the players in a team. The sport of football is especially tough for RL as it requires a characteristic harmony between short-term control, learned ideas, like passing, and high-level technique. Plus this environment, can also be used for other RL research topics like impact of stochasticity, self-play, multi-agent setups and model-based reinforcement learning, while also requiring smart decisions, tactics, and strategies at multiple levels of abstraction.</p>

<table style="margin-left:auto;margin-right:auto;">
<caption style="text-align:left"> Simulation of the Environment which shows different scenarios in which a policy scores a goal.</caption>
<tr>
<td>

<figure class="video_container">
  <iframe src="https://1.bp.blogspot.com/--ZLpcuDy8cg/XPqSR94pTsI/AAAAAAAAEMw/kx7V--J0oMMiA1wxjlCvNna9TISsafANgCLcBGAs/s1600/image9.gif" frameborder="0" allowfullscreen="true"> </iframe>
</figure>

</td>
<td>

<figure class="video_container">
  <iframe src="https://1.bp.blogspot.com/-TUBhhZ-HpQ4/XPqSRu7enTI/AAAAAAAAEMs/VBSMPbkOM4cXnJvTwX5fs9wQDC8iATYAACEwYBhgL/s1600/image8.gif" frameborder="0" allowfullscreen="true"> </iframe>
</figure>

</td>
</tr>

<tr>
<td>

<figure class="video_container">
  <iframe src="https://1.bp.blogspot.com/-M2owLueZyFc/XPqSVOEsKaI/AAAAAAAAEM0/KbVdRT5BTlYK1f7gfc0AhAGHb4EKyOZ2wCEwYBhgL/s1600/image4.gif" frameborder="0" allowfullscreen="true"> </iframe>
</figure>

</td>
<td>

<figure class="video_container">
  <iframe src="https://1.bp.blogspot.com/-HkcNiCL13cc/XPqSVOgTwMI/AAAAAAAAEM4/OoK_qoM14QA6VNQ79sWeS97TKBhCD7CzQCLcBGAs/s1600/image3.gif" frameborder="0" allowfullscreen="true"> </iframe>
</figure>

</td>
</tr>
</table>

<h2 id="environment-details">Environment Details</h2>
<h4 id="observation-space">Observation Space</h4>
<p>A <strong>State</strong> in this environment consists of various kind of information like ball position and possession, coordinates of all players, the active player, tiredness levels of players, yellow cards, score and the current pixel frame e.t.c. There are 3 ways in which an <strong>observation</strong> for this state can be represented:</p>
<ul>
    <li> <strong>Pixels</strong>: a 1280x720 RGB image representing the rendered image. </li>
    <li> <strong>Super Mini Map (SMM)</strong>: Consists of four 72x96 matrices which provide information about the home team, away team, ball and the active player. </li>
    <li> <strong>Floats</strong>: a 115 dimensional vector which provides information like player coordinates, ball possession and direction, active player, or game mode. </li>
</ul>

<h4 id="action-space">Action Space</h4>
<p>There are 19 different actions that can be performed by a player. These include move actions in 8 directions, different ways of kicking the ball, dribbling, tacking e.t.c. Detailed information regarding the action space can be found <a href="https://github.com/google-research/football/blob/master/gfootball/doc/observation.md">here</a>.</p>

<h4 id="rewards">Rewards</h4>
<p>There are 2 reward functions that are provided in this environment. They are as follows:</p>
<ul>
<li><strong>Scoring</strong>: In this reward function, the agent’s team gets a reward of +1 if it scores a goal and a reward of -1 if it concedes a goal to the opponent team.</li>
<li><strong>Checkpoint</strong>: This reward function divides the opponent team into 10 checkpoints and awards a reward of +0.1 if an agent is in one of these regions. This reward can go upto +1. Thus, this reward incentivizes the agent’s team to move towards opponent’s goal.</li>
</ul>

<h2 id="opponent-ai">Opponent AI</h2>
<p>There is a rule-based bot provided with the environment which can be used to train our RL algorithms. The difficulty of this bot is controlled by a parameter \(\theta\) that varies from 0 to 1. Changing this parameter influences the reaction time and decision making of the opponent team. Typically, these \(\theta\) values correspond to different difficulty levels:</p>
<ol>
  <li>Easy (\(\theta\)=0.05)</li>
  <li>Hard (\(\theta\)=0.95)</li>
</ol>

<p>We did our experiments on Easy setting.</p>

<h2 id="game-modes">Game Modes</h2>
<p>There are 2 modes in which we can train our RL agent:</p>
<ol>
  <li><strong>Single Agent</strong>: The player closest to the ball is the active player and our RL algorithm controls the actions of this player. While this happens, the other players in our team move based on a built-in heuristic.</li>
  <li><strong>Multi Agent</strong>: In this setting, each player can be controlled separately. The goal here is to train a set of agents from a random policy to a cooperating team.</li>
</ol>

<p>Our experiments our limited to Single-Agent setting for now. Our future work involves extending to a multi agent setting.</p>

<h2 id="baselines">Baselines</h2>
<p>We used 3 baselines for our experiments. They are as follows:</p>
<ol>
  <li><strong><a href="https://arxiv.org/abs/1707.06347">Proximal Policy Optimization (PPO)</a></strong></li>
  <li><strong><a href="https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf">Deep Q Network (DQN)</a></strong></li>
  <li><strong><a href="https://arxiv.org/pdf/1602.01783v2.pdf">Advantage Actor Critic (A2C)</a></strong></li>
</ol>

<p><a href="https://github.com/DLR-RM/stable-baselines3"><code class="language-plaintext highlighter-rouge">stable_baselines3</code></a> was used for the implementation of these algorithms. They were trained for a total of 20M timesteps. These were evaluated every 2048 timesteps and <code class="language-plaintext highlighter-rouge">n_eval_episodes=1</code>. On an average, it took around 4-4.5 days to train each algorithm. We used 3 different seeds for our experiments as well.</p>

<p>The hyperparameters used in our experiments are as follows:</p>
<table>
<tr>
<th style="text-align:center"> PPO </th>
<th style="text-align:center"> DQN </th>
<th style="text-align:center"> A2C </th>
</tr>

<tr>
<td>
<ul>
    <li>learning_rate=0.000343</li>
    <li>n_steps= 512</li>
    <li>batch_size=32</li>
    <li>n_epochs=2</li>
    <li>gamma=0.993</li>
    <li>gae_lambda=0.95</li>
    <li>clip_range=0.08</li> 
    <li>ent_coef=0.003</li> 
    <li>vf_coef=0.5</li>
    <li>max_grad_norm=0.64</li>
</ul>
</td>

<td>
<ul>
    <li>learning_rate=0.00015</li>
    <li>buffer_size=50000</li>
    <li>learning_starts=512</li>
    <li>batch_size=32</li>
    <li>gamma=0.993</li>
    <li>train_freq=512</li>
    <li>max_grad_norm=0.64</li>
    <li>target_update_interval=512</li>
</ul>
</td>

<td>
<ul>
    <li>learning_rate=0.0005</li>
    <li>n_steps=512</li>
    <li>gamma=0.993</li>
    <li>gae_lambda=0.95</li>
    <li>ent_coef=0.003</li>
    <li>vf_coef=0.5</li>
    <li>max_grad_norm=0.64</li>
</ul>
</td>
</tr>
</table>

<h2 id="our-approach">Our Approach</h2>
<h4 id="feature-engineering">Feature Engineering</h4>
<ol>
  <li><strong>Environment</strong>: We decided to use the Float-115 representation. We extended this representation to include the relative position and direction among teammates and opponents, active player and the ball. We also added sticky actions (info regarding active player), goalkeeper position and direction relative to the nearest player from both the same team and the opponent team. The resultant vector was 749 dimensional vector.</li>
  <li><strong>Reward</strong>: We reshaped the reward function to reduce the sparsity in rewards. For e.g, +0.1 for gaining ball possession and -0.1 for losing it. +0.2 for a successful slide and -0.2 for an unsuccessful one. +0.001 if the team holding the ball is ours otherwise -0.001. Still a lot of fine-tuning can be done in this, but we got decent results with this when compared to the baselines.</li>
</ol>

<h4 id="algorithm">Algorithm</h4>
<p>We used <strong><a href="https://arxiv.org/pdf/1802.01561.pdf">IMPALA</a></strong>, a distributed off-policy reinforcement algorithm as the core RL agent. We decided to use this algorithm to speed up the training process in comparison to the baselines. For our 749-dimensional observation space, we used a 8 layer MLP feature extractor. Google Research’s <a href="https://github.com/google-research/seed_rl"><code class="language-plaintext highlighter-rouge">Seed-RL</code></a> repository was utilized for implementing the IMPALA algorithm.</p>

<figure style="text-align:center">
<img class="img-fluid rounded z-depth-1" src="/assets/img/RL_Proj_Pics/Impala.jpeg" width="600" height="800" />
<figcaption style="text-align:center">Architecture of the IMPALA algorithm.</figcaption>
</figure>

<p>The hyperparameters used for training IMPALA are as follows:</p>
<ul>
    <li>Total_time_steps=20M</li>
    <li>batch_size: 256</li>
    <li>num_actors=4</li>
    <li>eval_freq=2048</li>
    <li>n_eval_episodes = 1</li>
    <li>learning_rate=0.00038</li>
    <li>gamma: 0.97</li>
    <li>Replay_Buffer Size=30000</li>
</ul>

<h2 id="results">Results</h2>
<p>The below graph shows the results of our experiment against an <code class="language-plaintext highlighter-rouge">11_v_11_Easy_Bot</code>. Our simplistic approach beat all the baselines. Result shown below is an average of 3 runs.</p>

<p><img class="img-fluid rounded z-depth-1" src="/assets/img/RL_Proj_Pics/DRL_Proj_Results.jpeg" width="600" height="800" /></p>

<p>The below <strong>video</strong> is the simulation of our policy against an easy bot.</p>

<figure class="video_container">
  <video controls="true" allowfullscreen="true" width="600" height="400">
    <source src="/assets/img/RL_Proj_Pics/football.mp4" type="video/mp4" />
  </video>
</figure>

<h4 id="few-observations">Few Observations</h4>
<ol>
  <li>Our agent preferred to dribble and proceed towards the opponent’s goal in order to score a goal. It didn’t learn a passing based collaborative strategy for scoring. This was probably due to the checkpoint rewards which incentivized an attacking based strategy.</li>
  <li>When training with a single agent, the policy being trained can only control one player at a time, while all the other players are controlled by the in-game AI (Easy teammates when opponent is Easy). It seems likely that if we train an agent with weaker teammates, it might outperform better agents if given a “smarter” team. This problem can also be fixed using multi agent training where all the players can be trained at once giving the agent full control.</li>
</ol>

<h2 id="future-work">Future Work</h2>
<p>We did not get a chance to try out few of the tricks to improve the agent’s performance. They are as follows:</p>
<ol>
  <li>Curriculum and Imitation Learning</li>
  <li>Self Play</li>
  <li>Combining RL agents with the rule based strategies, i.e learn an ensemble policy</li>
  <li>Adaptively changing difficulty to train our RL agents.</li>
</ol>

<h2 id="hardware">Hardware</h2>
<p>All of our experiments were run on <a href="https://sites.google.com/nyu.edu/nyu-hpc/hpc-systems/greene?authuser=0">NYU Greene HPC</a>. We used multiple nodes each consisting of 2 socket Intel Xeon Platinum 8268 24C 205W 2.9GHz Processor and 4x NVIDIA V100 GPUs.</p>

<h2 id="acknowledgement">Acknowledgement</h2>
<p>We want to thank Dr. <a href="https://www.lerrelpinto.com/">Lerrel Pinto</a> for teaching this wonderful course and also the TAs for overseeing the course logistics and handling our doubts.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Report for my Deep Reinforcement Learning Course (CSCI-GA 3033.090) Project at NYU]]></summary></entry></feed>