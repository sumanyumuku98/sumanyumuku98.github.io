---
---

@article{DBLP:journals/corr/abs-1912-01667,
  abbr={CoRR},
  author    = {Siddhant Bhambri and
               Sumanyu Muku and
               Avinash Tulasi and
               Arun Balaji Buduru},
  abstract={Machine learning has seen tremendous advances in the past few years, which has lead to deep learning models being deployed in varied applications of day-to-day life. Attacks on such models using perturbations, particularly in real-life scenarios, pose a severe challenge to their applicability, pushing research into the direction which aims to enhance the robustness of these models. After the introduction of these perturbations by Szegedy et al. [1], significant amount of research has focused on the reliability of such models, primarily in two aspects - white-box, where the adversary has access to the targeted model and related parameters; and the black-box, which resembles a real-life scenario with the adversary having almost no knowledge of the model to be attacked. To provide a comprehensive security cover, it is essential to identify, study, and build defenses against such attacks. Hence, in this paper, we propose to present a comprehensive comparative study of various black-box adversarial attacks and defense techniques.},
  bibtex_show={true},
  html={https://arxiv.org/abs/1912.01667},
  title     = {A Study of Black Box Adversarial Attacks in Computer Vision Models},
  journal   = {CoRR},
  volume    = {abs/1912.01667},
  year      = {2019},
  url       = {http://arxiv.org/abs/1912.01667},
  eprinttype = {arXiv},
  eprint    = {1912.01667},
  timestamp = {Sat, 23 Jan 2021 01:17:32 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1912-01667.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Rangarajan_2021,
  abbr={Eur. Radiol},
  abstract={To study whether a trained convolutional neural network (CNN) can be of assistance to radiologists in differentiating Coronavirus disease (COVID)â€“positive from COVID-negative patients using chest X-ray (CXR) through an ambispective clinical study. To identify subgroups of patients where artificial intelligence (AI) can be of particular value and analyse what imaging features may have contributed to the performance of AI by means of visualisation techniques.},
  bibtex_show={true},
  html={https://link.springer.com/article/10.1007/s00330-020-07628-5#Abs1},
	doi = {10.1007/s00330-020-07628-5},
	url = {https://doi.org/10.1007%2Fs00330-020-07628-5},
	year = 2021,
	month = {jan},
	publisher = {Springer Science and Business Media {LLC}},
	author = {Krithika Rangarajan and Sumanyu Muku and Amit Kumar Garg and Pavan Gabra and Sujay Halkur Shankar and Neeraj Nischal and Kapil Dev Soni and Ashu Seith Bhalla and Anant Mohan and Pawan Tiwari and Sushma Bhatnagar and Raghav Bansal and Atin Kumar and Shivanand Gamanagati and Richa Aggarwal and Upendra Baitha and Ashutosh Biswas and Arvind Kumar and Pankaj Jorwal and  Shalimar and A. Shariff and Naveet Wig and Rajeshwari Subramanium and Anjan Trikha and Rajesh Malhotra and Randeep Guleria and Vinay Namboodiri and Subhashis Banerjee and Chetan Arora},
	title = {Artificial Intelligence{\textendash}assisted chest X-ray assessment scheme for {COVID}-19},
	journal = {European Radiology}
}

@INPROCEEDINGS{sumanyu_wacv,
  abbr={WACV},
  author={Sharat Agarwal and Sumanyu Muku and Saket Anand and Chetan Arora},
  bibtex_show={true},
  abstract={Contextual information is a valuable cue for Deep Neural Networks (DNNs) to learn better representations and improve accuracy. However, co-occurrence bias in the training dataset may hamper a DNN model's generalizability to unseen scenarios in the real world. For example, in COCO, many object categories have a much higher co-occurrence with men compared to women, which can bias a DNN's prediction in favor of men. Recent works have focused on task-specific training strategies to handle bias in such scenarios, but fixing the available data is often ignored. In this paper, we propose a novel and more generic solution to address the contextual bias in the datasets by selecting a subset of the samples, which is fair in terms of the co-occurrence with various classes for a protected attribute. We introduce a data repair algorithm using the coefficient of variation, which can curate fair and contextually balanced data for a protected class(es). This helps in training a fair model irrespective of the task, architecture or training methodology. Our proposed solution is simple, effective, and can even be used in an active learning setting where the data labels are not present or being generated incrementally. We demonstrate the effectiveness of our algorithm for the task of object detection and multi-label image classification across different datasets. Through a series of experiments, we validate that curating contextually fair data helps make model predictions fair by balancing the true positive rate for the protected class across groups without compromising on the model's overall performance.},
  html={https://arxiv.org/abs/2110.10389},
  booktitle={2022 IEEE Winter Conference on Applications of Computer Vision (WACV)}, 

  title={Does Data Repair Lead to Fair Models? Curating Contextually Fair Data To Reduce Model Bias}, 

  year={2022},
  volume={},
  number={},
  pages={},

  }
