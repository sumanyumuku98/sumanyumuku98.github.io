<!DOCTYPE html>
<html>

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Sumanyu Muku


  | Feature Engineering + IMPALA for Google Research Football

</title>
<meta name="description" content="Sumanyu's Portfolio Page.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/blog/2021/DRL/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="http://localhost:4000/">
       Sumanyu Muku
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
          
          
          <!-- <li class="nav-item dropdown ">
              <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                submenus
                
              </a>
              <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
              
              
                <a class="dropdown-item" href="/publications/">publications</a>
              
              
              
                <div class="dropdown-divider"></div>
              
              
              
                <a class="dropdown-item" href="/projects/">projects</a>
              
              
              </div>
          </li> -->
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                teaching
                
              </a>
          </li>
          
          
          
          
            <div class="toggle-container">
              <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Feature Engineering + IMPALA for Google Research Football</h1>
    <p class="post-meta">December 13, 2021</p>
  </header>

  <article class="post-content">
    <h2 id="introduction">Introduction</h2>
<p>Google Research released their <a href="https://ai.googleblog.com/2019/06/introducing-google-research-football.html" target="_blank" rel="noopener noreferrer">Football Environment</a> in 2019 for research in Deep Reinforcement Learning (RL). The environment provides a realistic 3D simulation of the game of football in which the agents can control either a single player or all the players in a team. The sport of football is especially tough for RL as it requires a characteristic harmony between short-term control, learned ideas, like passing, and high-level technique. Plus this environment, can also be used for other RL research topics like impact of stochasticity, self-play, multi-agent setups and model-based reinforcement learning, while also requiring smart decisions, tactics, and strategies at multiple levels of abstraction.</p>

<table style="margin-left:auto;margin-right:auto;">
<caption style="text-align:left"> Simulation of the Environment which shows different scenarios in which a policy scores a goal.</caption>
<tr>
<td>

<figure class="video_container">
  <iframe src="https://1.bp.blogspot.com/--ZLpcuDy8cg/XPqSR94pTsI/AAAAAAAAEMw/kx7V--J0oMMiA1wxjlCvNna9TISsafANgCLcBGAs/s1600/image9.gif" frameborder="0" allowfullscreen="true"> </iframe>
</figure>

</td>
<td>

<figure class="video_container">
  <iframe src="https://1.bp.blogspot.com/-TUBhhZ-HpQ4/XPqSRu7enTI/AAAAAAAAEMs/VBSMPbkOM4cXnJvTwX5fs9wQDC8iATYAACEwYBhgL/s1600/image8.gif" frameborder="0" allowfullscreen="true"> </iframe>
</figure>

</td>
</tr>

<tr>
<td>

<figure class="video_container">
  <iframe src="https://1.bp.blogspot.com/-M2owLueZyFc/XPqSVOEsKaI/AAAAAAAAEM0/KbVdRT5BTlYK1f7gfc0AhAGHb4EKyOZ2wCEwYBhgL/s1600/image4.gif" frameborder="0" allowfullscreen="true"> </iframe>
</figure>

</td>
<td>

<figure class="video_container">
  <iframe src="https://1.bp.blogspot.com/-HkcNiCL13cc/XPqSVOgTwMI/AAAAAAAAEM4/OoK_qoM14QA6VNQ79sWeS97TKBhCD7CzQCLcBGAs/s1600/image3.gif" frameborder="0" allowfullscreen="true"> </iframe>
</figure>

</td>
</tr>
</table>

<h2 id="environment-details">Environment Details</h2>
<h4 id="observation-space">Observation Space</h4>
<p>A <strong>State</strong> in this environment consists of various kind of information like ball position and possession, coordinates of all players, the active player, tiredness levels of players, yellow cards, score and the current pixel frame e.t.c. There are 3 ways in which an <strong>observation</strong> for this state can be represented:</p>
<ul>
    <li> <strong>Pixels</strong>: a 1280x720 RGB image representing the rendered image. </li>
    <li> <strong>Super Mini Map (SMM)</strong>: Consists of four 72x96 matrices which provide information about the home team, away team, ball and the active player. </li>
    <li> <strong>Floats</strong>: a 115 dimensional vector which provides information like player coordinates, ball possession and direction, active player, or game mode. </li>
</ul>

<h4 id="action-space">Action Space</h4>
<p>There are 19 different actions that can be performed by a player. These include move actions in 8 directions, different ways of kicking the ball, dribbling, tacking e.t.c. Detailed information regarding the action space can be found <a href="https://github.com/google-research/football/blob/master/gfootball/doc/observation.md" target="_blank" rel="noopener noreferrer">here</a>.</p>

<h4 id="rewards">Rewards</h4>
<p>There are 2 reward functions that are provided in this environment. They are as follows:</p>
<ul>
<li>
<strong>Scoring</strong>: In this reward function, the agent’s team gets a reward of +1 if it scores a goal and a reward of -1 if it concedes a goal to the opponent team.</li>
<li>
<strong>Checkpoint</strong>: This reward function divides the opponent team into 10 checkpoints and awards a reward of +0.1 if an agent is in one of these regions. This reward can go upto +1. Thus, this reward incentivizes the agent’s team to move towards opponent’s goal.</li>
</ul>

<h2 id="opponent-ai">Opponent AI</h2>
<p>There is a rule-based bot provided with the environment which can be used to train our RL algorithms. The difficulty of this bot is controlled by a parameter \(\theta\) that varies from 0 to 1. Changing this parameter influences the reaction time and decision making of the opponent team. Typically, these \(\theta\) values correspond to different difficulty levels:</p>
<ol>
  <li>Easy (\(\theta\)=0.05)</li>
  <li>Hard (\(\theta\)=0.95)</li>
</ol>

<p>We did our experiments on Easy setting.</p>

<h2 id="game-modes">Game Modes</h2>
<p>There are 2 modes in which we can train our RL agent:</p>
<ol>
  <li>
<strong>Single Agent</strong>: The player closest to the ball is the active player and our RL algorithm controls the actions of this player. While this happens, the other players in our team move based on a built-in heuristic.</li>
  <li>
<strong>Multi Agent</strong>: In this setting, each player can be controlled separately. The goal here is to train a set of agents from a random policy to a cooperating team.</li>
</ol>

<p>Our experiments our limited to Single-Agent setting for now. Our future work involves extending to a multi agent setting.</p>

<h2 id="baselines">Baselines</h2>
<p>We used 3 baselines for our experiments. They are as follows:</p>
<ol>
  <li><strong><a href="https://arxiv.org/abs/1707.06347" target="_blank" rel="noopener noreferrer">Proximal Policy Optimization (PPO)</a></strong></li>
  <li><strong><a href="https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf" target="_blank" rel="noopener noreferrer">Deep Q Network (DQN)</a></strong></li>
  <li><strong><a href="https://arxiv.org/pdf/1602.01783v2.pdf" target="_blank" rel="noopener noreferrer">Advantage Actor Critic (A2C)</a></strong></li>
</ol>

<p><a href="https://github.com/DLR-RM/stable-baselines3" target="_blank" rel="noopener noreferrer"><code class="language-plaintext highlighter-rouge">stable_baselines3</code></a> was used for the implementation of these algorithms. They were trained for a total of 20M timesteps. These were evaluated every 2048 timesteps and <code class="language-plaintext highlighter-rouge">n_eval_episodes=1</code>. On an average, it took around 4-4.5 days to train each algorithm. We used 3 different seeds for our experiments as well.</p>

<p>The hyperparameters used in our experiments are as follows:</p>
<table>
<tr>
<th style="text-align:center"> PPO </th>
<th style="text-align:center"> DQN </th>
<th style="text-align:center"> A2C </th>
</tr>

<tr>
<td>
<ul>
    <li>learning_rate=0.000343</li>
    <li>n_steps= 512</li>
    <li>batch_size=32</li>
    <li>n_epochs=2</li>
    <li>gamma=0.993</li>
    <li>gae_lambda=0.95</li>
    <li>clip_range=0.08</li> 
    <li>ent_coef=0.003</li> 
    <li>vf_coef=0.5</li>
    <li>max_grad_norm=0.64</li>
</ul>
</td>

<td>
<ul>
    <li>learning_rate=0.00015</li>
    <li>buffer_size=50000</li>
    <li>learning_starts=512</li>
    <li>batch_size=32</li>
    <li>gamma=0.993</li>
    <li>train_freq=512</li>
    <li>max_grad_norm=0.64</li>
    <li>target_update_interval=512</li>
</ul>
</td>

<td>
<ul>
    <li>learning_rate=0.0005</li>
    <li>n_steps=512</li>
    <li>gamma=0.993</li>
    <li>gae_lambda=0.95</li>
    <li>ent_coef=0.003</li>
    <li>vf_coef=0.5</li>
    <li>max_grad_norm=0.64</li>
</ul>
</td>
</tr>
</table>

<h2 id="our-approach">Our Approach</h2>
<h4 id="feature-engineering">Feature Engineering</h4>
<ol>
  <li>
<strong>Environment</strong>: We decided to use the Float-115 representation. We extended this representation to include the relative position and direction among teammates and opponents, active player and the ball. We also added sticky actions (info regarding active player), goalkeeper position and direction relative to the nearest player from both the same team and the opponent team. The resultant vector was 749 dimensional vector.</li>
  <li>
<strong>Reward</strong>: We reshaped the reward function to reduce the sparsity in rewards. For e.g, +0.1 for gaining ball possession and -0.1 for losing it. +0.2 for a successful slide and -0.2 for an unsuccessful one. +0.001 if the team holding the ball is ours otherwise -0.001. Still a lot of fine-tuning can be done in this, but we got decent results with this when compared to the baselines.</li>
</ol>

<h4 id="algorithm">Algorithm</h4>
<p>We used <strong><a href="https://arxiv.org/pdf/1802.01561.pdf" target="_blank" rel="noopener noreferrer">IMPALA</a></strong>, a distributed off-policy reinforcement algorithm as the core RL agent. We decided to use this algorithm to speed up the training process in comparison to the baselines. For our 749-dimensional observation space, we used a 8 layer MLP feature extractor. Google Research’s <a href="https://github.com/google-research/seed_rl" target="_blank" rel="noopener noreferrer"><code class="language-plaintext highlighter-rouge">Seed-RL</code></a> repository was utilized for implementing the IMPALA algorithm.</p>

<figure style="text-align:center">
<img class="img-fluid rounded z-depth-1" src="/assets/img/RL_Proj_Pics/Impala.jpeg" width="600" height="800">
<figcaption style="text-align:center">Architecture of the IMPALA algorithm.</figcaption>
</figure>

<p>The hyperparameters used for training IMPALA are as follows:</p>
<ul>
    <li>Total_time_steps=20M</li>
    <li>batch_size: 256</li>
    <li>num_actors=4</li>
    <li>eval_freq=2048</li>
    <li>n_eval_episodes = 1</li>
    <li>learning_rate=0.00038</li>
    <li>gamma: 0.97</li>
    <li>Replay_Buffer Size=30000</li>
</ul>

<h2 id="results">Results</h2>
<p>The below graph shows the results of our experiment against an <code class="language-plaintext highlighter-rouge">11_v_11_Easy_Bot</code>. Our simplistic approach beat all the baselines. Result shown below is an average of 3 runs.</p>

<p><img class="img-fluid rounded z-depth-1" src="/assets/img/RL_Proj_Pics/DRL_Proj_Results.jpeg" width="600" height="800"></p>

<p>The below <strong>video</strong> is the simulation of our policy against an easy bot.</p>

<figure class="video_container">
  <video controls="true" allowfullscreen="true" width="600" height="400">
    <source src="/assets/img/RL_Proj_Pics/football.mp4" type="video/mp4"></source>
  </video>
</figure>

<h4 id="few-observations">Few Observations</h4>
<ol>
  <li>Our agent preferred to dribble and proceed towards the opponent’s goal in order to score a goal. It didn’t learn a passing based collaborative strategy for scoring. This was probably due to the checkpoint rewards which incentivized an attacking based strategy.</li>
  <li>When training with a single agent, the policy being trained can only control one player at a time, while all the other players are controlled by the in-game AI (Easy teammates when opponent is Easy). It seems likely that if we train an agent with weaker teammates, it might outperform better agents if given a “smarter” team. This problem can also be fixed using multi agent training where all the players can be trained at once giving the agent full control.</li>
</ol>

<h2 id="future-work">Future Work</h2>
<p>We did not get a chance to try out few of the tricks to improve the agent’s performance. They are as follows:</p>
<ol>
  <li>Curriculum and Imitation Learning</li>
  <li>Self Play</li>
  <li>Combining RL agents with the rule based strategies, i.e learn an ensemble policy</li>
  <li>Adaptively changing difficulty to train our RL agents.</li>
</ol>

<h2 id="hardware">Hardware</h2>
<p>All of our experiments were run on <a href="https://sites.google.com/nyu.edu/nyu-hpc/hpc-systems/greene?authuser=0" target="_blank" rel="noopener noreferrer">NYU Greene HPC</a>. We used multiple nodes each consisting of 2 socket Intel Xeon Platinum 8268 24C 205W 2.9GHz Processor and 4x NVIDIA V100 GPUs.</p>

<h2 id="acknowledgement">Acknowledgement</h2>
<p>We want to thank Dr. <a href="https://www.lerrelpinto.com/" target="_blank" rel="noopener noreferrer">Lerrel Pinto</a> for teaching this wonderful course and also the TAs for overseeing the course logistics and handling our doubts.</p>

  </article>

  

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    © Copyright 2024 Sumanyu  Muku.
    Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme.

    
    
    Last updated: January 19, 2024.
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
